{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from models import main_models\n",
    "\n",
    "import numpy as np\n",
    "import datasets\n",
    "from datasets import dataset\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "parser=argparse.ArgumentParser()\n",
    "parser.add_argument('--n_epoches_1',type=int,default= 50)\n",
    "parser.add_argument('--n_epoches_2',type=int,default= 100)\n",
    "parser.add_argument('--n_epoches_3',type=int,default=200)\n",
    "parser.add_argument('--n_target_samples',type=int,default=7)\n",
    "parser.add_argument('--batch_size',type=int,default= 256)\n",
    "\n",
    "opt=vars(parser.parse_args())\n",
    "\n",
    "use_cuda=True if torch.cuda.is_available() else False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(1)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(1)\n",
    "\n",
    "    \n",
    "#--------------pretrain g and h for step 1---------------------------------\n",
    "species_all = ['Homo_sapiens','Rattus_norvegicus','Schistosoma_japonicum','Saccharomyces_cerevisiae','Mus_musculus','Escherichia_coli','Bacillus_velezensis','Plasmodium_falciparum','Oryza_sativa','Arabidopsis_thaliana']\n",
    "specie1 = species_all[0]# Homo sapiens\n",
    "specie2 = species_all[1]# Rattus norvegicus\n",
    "\n",
    "result_path = '/harddisk/hdd_d/liuyu/PTM/ace/PTM_GAN/results/'\n",
    "if not os.path.exists(result_path):\n",
    "        os.makedirs(result_path)\n",
    "        \n",
    "model_path = '/harddisk/hdd_d/liuyu/PTM/ace/PTM_GAN/results/'\n",
    "if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "        \n",
    "mainfolder = '/harddisk/hdd_d/liuyu/PTM/ace/PTM_GAN/result/GAN/' + specie2 + '/' \n",
    "if not os.path.exists(mainfolder):\n",
    "    os.makedirs(mainfolder)\n",
    "figfolder = mainfolder + '/figure'\n",
    "if not os.path.exists(figfolder):\n",
    "    os.makedirs(figfolder)\n",
    "prefolder = mainfolder + '/prediction'\n",
    "if not os.path.exists(prefolder):\n",
    "    os.makedirs(prefolder)\n",
    "feafolder = mainfolder + '/feature'\n",
    "if not os.path.exists(feafolder):\n",
    "    os.makedirs(feafolder)\n",
    "modelfolder = mainfolder + '/model'\n",
    "if not os.path.exists(modelfolder):\n",
    "    os.makedirs(modelfolder)\n",
    "\n",
    "data_path = '/harddisk/hdd_d/liuyu/PTM/ace/PTM_GAN/data_npy/'\n",
    "batch_size=opt['batch_size']\n",
    "test_CRC_data = dataset(data_path,specie1, mode = 'test')\n",
    "test_data_loader = torch.utils.data.DataLoader(test_CRC_data, batch_size=batch_size, shuffle=True) \n",
    "\n",
    "train_CRC_data = dataset(data_path,specie1, mode = 'train')\n",
    "train_data_loader = torch.utils.data.DataLoader(train_CRC_data, batch_size=batch_size, shuffle=True) \n",
    "validation_CRC_data = dataset(data_path,specie1, mode = 'valid')\n",
    "validation_data_loader = torch.utils.data.DataLoader(validation_CRC_data, batch_size=batch_size, shuffle=True) \n",
    "\n",
    "def plot_loss(y):\n",
    "\n",
    "    x = range(0,len(y))\n",
    "    plt.plot(x, y, '.-',color=\"red\")\n",
    "    plt_title = 'xxx'\n",
    "    plt.title(plt_title)\n",
    "    plt.xlabel('per 200 times')\n",
    "    plt.ylabel('LOSS')\n",
    "    plt.savefig(result_path+'ptm-{:s}-loss.png'.format('HUMAN'))\n",
    "    \n",
    "def save_feature(feature,phase,index):\n",
    "    l = feature.shape[1]\n",
    "    num = feature.shape[0]\n",
    "    final_matrix = np.ones((num,l))\n",
    "    for i in range(num):\n",
    "        temp = feature[i].flatten()\n",
    "        final_matrix[i] = temp\n",
    "    np.savetxt(feafolder + '/feature_{:s}_{:d}.txt'.format(phase,index),final_matrix)\n",
    "    \n",
    "    \n",
    "classifier=main_models.Classifier()\n",
    "encoder=main_models.Encoder()\n",
    "discriminator=main_models.DCD(input_features=128)\n",
    "\n",
    "#print('net--------------------------')\n",
    "#print(encoder)\n",
    "\n",
    "classifier.to(device)\n",
    "encoder.to(device)\n",
    "discriminator.to(device)\n",
    "loss_fn=torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer=torch.optim.Adam(list(encoder.parameters())+list(classifier.parameters()),lr=0.0001,weight_decay=0.0001)\n",
    "\n",
    "Loss = []\n",
    "for epoch in range(opt['n_epoches_1']):\n",
    "    if epoch == 50:\n",
    "        optimizer=torch.optim.Adam(list(encoder.parameters())+list(classifier.parameters()),lr=0.00002,weight_decay=0.0001)\n",
    "    for i, (data, labels) in enumerate(train_data_loader):\n",
    "        data=data.to(device)\n",
    "        labels = labels[:,1]\n",
    "        labels=labels.to(device)\n",
    "        data = Variable(data,requires_grad=True)\n",
    "        labels = Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred=classifier(encoder(data))\n",
    "        labels = labels.long()\n",
    "        loss=loss_fn(y_pred,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    acc=0\n",
    "    auc = 0\n",
    "    y_test_pred_all = 0\n",
    "    labels_all = 0\n",
    "    for i, (data, labels) in enumerate(test_data_loader):\n",
    "        \n",
    "        data=data.to(device)\n",
    "        labels = labels[:,1]\n",
    "        labels=labels.to(device)\n",
    "        labels = labels.long()\n",
    "        y_test_pred=classifier(encoder(data))\n",
    "        loss=loss_fn(y_test_pred,labels)\n",
    "        running_loss = loss.item()\n",
    "        Loss.append(running_loss)\n",
    "        \n",
    "        if i == 0:\n",
    "            y_test_pred_all = y_test_pred.detach().cpu()\n",
    "            labels_all = labels.detach().cpu()\n",
    "        else:\n",
    "            y_test_pred_all = torch.cat((y_test_pred_all,y_test_pred.detach().cpu()), 0)\n",
    "            labels_all = torch.cat((labels_all,labels.detach().cpu()), 0)        \n",
    "\n",
    "        acc+=(torch.max(y_test_pred.detach().cpu(),1)[1]==labels.detach().cpu()).float().mean().item()\n",
    "        \n",
    "    #y_test_pred_all = y_test_pred_all.detach().cpu()\n",
    "    #labels_all = labels_all.detach().cpu()\n",
    "    #print(labels_all)\n",
    "    plot_loss(Loss)\n",
    "    accuracy=round(acc / float(i+1), 3)\n",
    "    auc = metrics.roc_auc_score(labels_all,y_test_pred_all[:, 1])\n",
    "    print(\"step1----Epoch %d/%d  test accuracy: %.3f \"%(epoch+1,opt['n_epoches_1'],accuracy))\n",
    "    print(\"step1----Epoch %d/%d  test auc : %.3f \"%(epoch+1,opt['n_epoches_1'],auc))\n",
    "    \n",
    "    fpr_t, tpr_t, _ = metrics.roc_curve(labels_all,y_test_pred_all[:, 1])\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(fpr_t, tpr_t, 'b-', label='CNN-test {:.3%}'.format(auc))\n",
    "    ax.legend(loc='lower right', shadow=True)\n",
    "    plt.title('test of {:s}'.format('HUMAN'))\n",
    "    #plt.savefig(result_path+'ptm-{:s}-epoch-{:d}-auc-{:.4f}-acc-{:.4f}-test.png'.format('HUMAN', epoch+1, auc, accuracy))\n",
    "    plt.close()\n",
    "    torch.save(encoder, model_path + 'encoder1.pth.tar') \n",
    "    torch.save(classifier, model_path + 'classifier1.pth.tar') \n",
    "               \n",
    "    \n",
    "X_s,Y_s=datasets.sample_data(data_path,specie1, mode = 'train')\n",
    "X_t,Y_t=datasets.create_target_samples(data_path,specie2, mode = 'train')  \n",
    "#-----------------train DCD for step 2--------------------------------\n",
    "optimizer_D=torch.optim.Adam(discriminator.parameters(),lr=0.01)\n",
    "\n",
    "for epoch in range(opt['n_epoches_2']):\n",
    "    # data\n",
    "    groups,aa = datasets.sample_groups(X_s,Y_s,X_t,Y_t,seed=epoch)\n",
    "\n",
    "    n_iters = 4 * len(groups[1])\n",
    "    \n",
    "    \n",
    "    index_list = torch.randperm(n_iters)\n",
    "    mini_batch_size=40 #use mini_batch train can be more stable\n",
    "\n",
    "    loss_mean=[]\n",
    "\n",
    "    X1=[];X2=[];ground_truths=[]\n",
    "    for index in range(n_iters):\n",
    "\n",
    "        ground_truth=index_list[index]//len(groups[1])\n",
    "        x1,x2=groups[ground_truth][index_list[index]-len(groups[1])*ground_truth]\n",
    "        X1.append(x1)\n",
    "        X2.append(x2)\n",
    "        ground_truths.append(ground_truth)\n",
    "        \n",
    "        #select data for a mini-batch to train\n",
    "        if (index+1)%mini_batch_size==0:\n",
    "            X1=torch.stack(X1)\n",
    "            X2=torch.stack(X2)\n",
    "            ground_truths=torch.LongTensor(ground_truths)\n",
    "            X1=X1.to(device)\n",
    "            X2=X2.to(device)\n",
    "            ground_truths=ground_truths.to(device)\n",
    "            optimizer_D.zero_grad()\n",
    "            X_cat=torch.cat([encoder(X1),encoder(X2)],1)\n",
    "            y_pred=discriminator(X_cat.detach())\n",
    "            \n",
    "            loss=loss_fn(y_pred,ground_truths)\n",
    "            loss.backward()\n",
    "            optimizer_D.step()\n",
    "            loss_mean.append(loss.item())\n",
    "            X1 = []\n",
    "            X2 = []\n",
    "            ground_truths = []\n",
    "\n",
    "    print(\"step2----Epoch %d/%d loss:%.3f\"%(epoch+1,opt['n_epoches_2'],np.mean(loss_mean)))\n",
    "\n",
    "\n",
    "\n",
    "encoder=torch.load(model_path + 'encoder1.pth.tar')\n",
    "classifier=torch.load(model_path + 'classifier1.pth.tar')\n",
    "#-------------------training for step 3-------------------\n",
    "optimizer_g_h=torch.optim.Adam(list(encoder.parameters())+list(classifier.parameters()),lr=0.00001)\n",
    "optimizer_d=torch.optim.Adam(discriminator.parameters(),lr=0.00000001)\n",
    "\n",
    "\n",
    "test_CRC_data = dataset(data_path,specie2, mode = 'test')\n",
    "test_data_loader = torch.utils.data.DataLoader(test_CRC_data, batch_size=batch_size, shuffle=True) \n",
    "\n",
    "for epoch in range(opt['n_epoches_3']):\n",
    "    #---training g and h , DCD is frozen\n",
    "\n",
    "    groups, groups_y = datasets.sample_groups(X_s,Y_s,X_t,Y_t,seed=opt['n_epoches_2']+epoch)\n",
    "    G1, G2, G3, G4 = groups\n",
    "    Y1, Y2, Y3, Y4 = groups_y\n",
    "    groups_2 = [G2, G4]\n",
    "    groups_y_2 = [Y2, Y4]\n",
    "\n",
    "    n_iters = 2 * len(G2)\n",
    "    index_list = torch.randperm(n_iters)\n",
    "\n",
    "    n_iters_dcd = 4 * len(G2)\n",
    "    index_list_dcd = torch.randperm(n_iters_dcd)\n",
    "\n",
    "    mini_batch_size_g_h = 20 #data only contains G2 and G4 ,so decrease mini_batch\n",
    "    mini_batch_size_dcd= 40 #data contains G1,G2,G3,G4 so use 40 as mini_batch\n",
    "    X1 = []\n",
    "    X2 = []\n",
    "    ground_truths_y1 = []\n",
    "    ground_truths_y2 = []\n",
    "    dcd_labels=[]\n",
    "    for index in range(n_iters):\n",
    "\n",
    "\n",
    "        ground_truth=index_list[index]//len(G2)\n",
    "        x1, x2 = groups_2[ground_truth][index_list[index] - len(G2) * ground_truth]\n",
    "        y1, y2 = groups_y_2[ground_truth][index_list[index] - len(G2) * ground_truth]\n",
    "        # y1=torch.LongTensor([y1.item()])\n",
    "        # y2=torch.LongTensor([y2.item()])\n",
    "        \n",
    "        dcd_label=0 if ground_truth==0 else 2\n",
    "        X1.append(x1)\n",
    "        X2.append(x2)\n",
    "        ground_truths_y1.append(y1)\n",
    "        ground_truths_y2.append(y2)\n",
    "        dcd_labels.append(dcd_label)\n",
    "        \n",
    "       \n",
    "        if (index+1)%mini_batch_size_g_h==0:\n",
    "            ground_truths_y1 = torch.stack(ground_truths_y1)\n",
    "            ground_truths_y2 = torch.stack(ground_truths_y2)\n",
    "            ground_truths_y1 = ground_truths_y1[:,1]\n",
    "            ground_truths_y2 = ground_truths_y2[:,1]\n",
    "            X1=torch.stack(X1)\n",
    "            X2=torch.stack(X2)\n",
    "            ground_truths_y1=torch.Tensor(ground_truths_y1)\n",
    "            ground_truths_y2 = torch.Tensor(ground_truths_y2)\n",
    "            dcd_labels=torch.LongTensor(dcd_labels)\n",
    "            X1=X1.to(device)\n",
    "            X2=X2.to(device)\n",
    "            ground_truths_y1=ground_truths_y1.to(device)\n",
    "            ground_truths_y2 = ground_truths_y2.to(device)\n",
    "            dcd_labels=dcd_labels.to(device)\n",
    "\n",
    "            optimizer_g_h.zero_grad()\n",
    "\n",
    "            encoder_X1=encoder(X1)\n",
    "            encoder_X2=encoder(X2)\n",
    "\n",
    "            X_cat=torch.cat([encoder_X1,encoder_X2],1)\n",
    "            y_pred_X1=classifier(encoder_X1)\n",
    "            y_pred_X2=classifier(encoder_X2)\n",
    "            y_pred_dcd=discriminator(X_cat)\n",
    "            ground_truths_y1 = ground_truths_y1.long()\n",
    "            ground_truths_y2 = ground_truths_y2.long()\n",
    "            loss_X1=loss_fn(y_pred_X1,ground_truths_y1)\n",
    "            loss_X2=loss_fn(y_pred_X2,ground_truths_y2)\n",
    "            loss_dcd=loss_fn(y_pred_dcd,dcd_labels)\n",
    "\n",
    "            loss_sum = loss_X1 + loss_X2 + 0.2 * loss_dcd\n",
    "\n",
    "            loss_sum.backward()\n",
    "            optimizer_g_h.step()\n",
    "\n",
    "            X1 = []\n",
    "            X2 = []\n",
    "            ground_truths_y1 = []\n",
    "            ground_truths_y2 = []\n",
    "            dcd_labels = []\n",
    "\n",
    "\n",
    "    #----training dcd ,g and h frozen\n",
    "    X1 = []\n",
    "    X2 = []\n",
    "    ground_truths = []\n",
    "    for index in range(n_iters_dcd):\n",
    "\n",
    "        ground_truth=index_list_dcd[index]//len(groups[1])\n",
    "\n",
    "        x1, x2 = groups[ground_truth][index_list_dcd[index] - len(groups[1]) * ground_truth]\n",
    "        X1.append(x1)\n",
    "        X2.append(x2)\n",
    "        ground_truths.append(ground_truth)\n",
    "\n",
    "        if (index + 1) % mini_batch_size_dcd == 0:\n",
    "            X1 = torch.stack(X1)\n",
    "            X2 = torch.stack(X2)\n",
    "            ground_truths = torch.LongTensor(ground_truths)\n",
    "            X1 = X1.to(device)\n",
    "            X2 = X2.to(device)\n",
    "            ground_truths = ground_truths.to(device)\n",
    "\n",
    "            optimizer_d.zero_grad()\n",
    "            X_cat = torch.cat([encoder(X1), encoder(X2)], 1)\n",
    "            y_pred = discriminator(X_cat.detach())\n",
    "            loss = loss_fn(y_pred, ground_truths)\n",
    "            loss.backward()\n",
    "            optimizer_d.step()\n",
    "            # loss_mean.append(loss.item())\n",
    "            X1 = []\n",
    "            X2 = []\n",
    "            ground_truths = []\n",
    "\n",
    "    #testing\n",
    "    acc=0\n",
    "    auc = 0\n",
    "    y_test_pred_all = 0\n",
    "    labels_all = 0\n",
    "    for i, (data, labels) in enumerate(test_data_loader):\n",
    "        \n",
    "        data=data.to(device)\n",
    "        labels = labels[:,1]\n",
    "        labels=labels.to(device)\n",
    "        labels= labels.long()\n",
    "        y_test_pred=classifier(encoder(data))\n",
    "        loss=loss_fn(y_test_pred,labels)\n",
    "        running_loss = loss.item()\n",
    "        Loss.append(running_loss)\n",
    "        plot_loss(Loss)\n",
    "        if i == 0:\n",
    "            y_test_pred_all = y_test_pred.detach().cpu()\n",
    "            labels_all = labels.detach().cpu()\n",
    "        else:\n",
    "            y_test_pred_all = torch.cat((y_test_pred_all,y_test_pred.detach().cpu()), 0)\n",
    "            labels_all = torch.cat((labels_all,labels.detach().cpu()), 0)        \n",
    "        a =torch.max(y_test_pred.detach().cpu(),1)[1]\n",
    "        b =torch.max(y_test_pred.detach().cpu(),1)[1]==labels.detach().cpu()\n",
    "        acc+=(torch.max(y_test_pred.detach().cpu(),1)[1]==labels.detach().cpu()).float().mean().item()\n",
    "    accuracy = round(acc / float(i+1), 3)  \n",
    "    auc = metrics.roc_auc_score(labels_all,y_test_pred_all[:, 1])\n",
    "    np.savetxt(prefolder+ '/prediction scores of epoch-{:d}.txt'.format(epoch),y_test_pred_all)\n",
    "    np.savetxt(prefolder+ '/label of epoch-{:d}.txt'.format(epoch),labels_all)\n",
    "    \n",
    "    print(\"step1----Epoch %d/%d  accuracy: %.3f \"%(epoch+1,opt['n_epoches_3'],accuracy))\n",
    "    print(\"step1----Epoch %d/%d  auc: %.3f \"%(epoch+1,opt['n_epoches_3'],auc))\n",
    "    torch.save(encoder, modelfolder + '/encoder'+ str(epoch)+'.pth.tar') \n",
    "    torch.save(classifier, modelfolder + '/classifier'+ str(epoch)+'.pth.tar') \n",
    "    \n",
    "    fpr_t, tpr_t, _ = metrics.roc_curve(labels_all,y_test_pred_all[:, 1])\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(fpr_t, tpr_t, 'b-', label='CNN-test {:.3%}'.format(auc))\n",
    "    ax.legend(loc='lower right', shadow=True)\n",
    "    plt.title('test of {:s}'.format(specie2))\n",
    "    plt.savefig(figfolder + '/ptm-{:s}-epoch-{:d}-auc-{:.4f}-acc-{:.4f}-test.png'.format(specie2, epoch, auc, accuracy))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
